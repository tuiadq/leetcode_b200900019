{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rkR7BLdoU-yf"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')  # punkt номын санг татаж авах\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-nnpRBTDWwHG"
      },
      "outputs": [],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('punkt_tab')  # punkt_tab номын санг татаж авах\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "FYMhzSucWsvg",
        "outputId": "702dce07-9c19-4bc5-f508-a1c110deb909"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step - accuracy: 0.5000 - loss: 1.3814\n",
            "Epoch 2/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 53ms/step - accuracy: 0.7500 - loss: 1.3746\n",
            "Epoch 3/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step - accuracy: 1.0000 - loss: 1.3677\n",
            "Epoch 4/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 154ms/step - accuracy: 1.0000 - loss: 1.3606\n",
            "Epoch 5/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 100ms/step - accuracy: 1.0000 - loss: 1.3533\n",
            "Epoch 6/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 124ms/step - accuracy: 1.0000 - loss: 1.3458\n",
            "Epoch 7/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 200ms/step - accuracy: 1.0000 - loss: 1.3380\n",
            "Epoch 8/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step - accuracy: 1.0000 - loss: 1.3299\n",
            "Epoch 9/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step - accuracy: 1.0000 - loss: 1.3212\n",
            "Epoch 10/10\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step - accuracy: 1.0000 - loss: 1.3121\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 184ms/step\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "No model was supplied, defaulted to facebook/bart-large-mnli and revision d7645e1 (https://huggingface.co/facebook/bart-large-mnli).\n",
            "Using a pipeline without specifying a model name and revision in production is not recommended.\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Текстийн ангилал: 3\n",
            "Серверийн алдаа: 405\n",
            "\n",
            "Тэмдэглэсэн текст:\n",
            " Дэд бүтэц нээж, хувийн байгуулалгуудаа дэмжиж байж, улсын төсвийн хөрөнгө оруулалтаар хийж байж бизнесийн байгууллагуудын ажиллах орчин нөхцөлийг бүрдүүлэх ёстой. Ингэж байж зөвхөн уул уурхайгаас бус, бусад салбараас орлого олох боломжтой болно.\n",
            "\n",
            "Хамгийн их давтагдсан 10 үг:\n",
            " [('байж', 3), ('дэд', 1), ('бтэц', 1), ('нээж', 1), ('хувийн', 1), ('байгуулалгуудаа', 1), ('дэмжиж', 1), ('улсын', 1), ('тсвийн', 1), ('хрнг', 1)]\n",
            "\n",
            "Бичвэрийн агуулга:\n",
            " {'sequence': 'Дэд бүтэц нээж, хувийн байгуулалгуудаа дэмжиж байж, улсын төсвийн хөрөнгө оруулалтаар хийж байж бизнесийн байгууллагуудын ажиллах орчин нөхцөлийг бүрдүүлэх ёстой. Ингэж байж зөвхөн уул уурхайгаас бус, бусад салбараас орлого олох боломжтой болно.', 'labels': ['улс төр', 'соёл урлаг', 'эдийн засаг', 'спорт', 'технологи'], 'scores': [0.689415454864502, 0.146449014544487, 0.08029932528734207, 0.04334735870361328, 0.04048886150121689]}\n",
            " * Serving Flask app '__main__'\n",
            " * Debug mode: on\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "INFO:werkzeug:\u001b[31m\u001b[1mWARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\u001b[0m\n",
            " * Running on http://127.0.0.1:5000\n",
            "INFO:werkzeug:\u001b[33mPress CTRL+C to quit\u001b[0m\n",
            "INFO:werkzeug: * Restarting with stat\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "from collections import Counter\n",
        "import re\n",
        "from nltk.tokenize import word_tokenize\n",
        "from transformers import pipeline\n",
        "from flask import Flask, request, jsonify\n",
        "import nltk\n",
        "\n",
        "nltk.download('punkt')  # punkt номын санг татаж авах\n",
        "\n",
        "# Текст оруулах\n",
        "text = \"\"\"Дэд бүтэц нээж, хувийн байгуулалгуудаа дэмжиж байж, улсын төсвийн хөрөнгө оруулалтаар хийж байж бизнесийн байгууллагуудын ажиллах орчин нөхцөлийг бүрдүүлэх ёстой. Ингэж байж зөвхөн уул уурхайгаас бус, бусад салбараас орлого олох боломжтой болно.\"\"\"\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import numpy as np\n",
        "\n",
        "# Текстийн өгөгдөл\n",
        "texts = [\"Эдийн засаг өсч байна\", \"Монголын соёл урлагийн талаар\", \"Шинжлэх ухааны шинэ ололтууд\", \"Улс төрийн асуудлууд\"]\n",
        "labels = [0, 1, 2, 3]  # Эдийн засаг, соёл урлаг, технологи, улс төр\n",
        "\n",
        "# Токенжуулалт хийх\n",
        "tokenizer = Tokenizer(num_words=1000)\n",
        "tokenizer.fit_on_texts(texts)\n",
        "sequences = tokenizer.texts_to_sequences(texts)\n",
        "padded_sequences = pad_sequences(sequences, padding='post')\n",
        "\n",
        "# Загвар бүтээх\n",
        "model = models.Sequential([\n",
        "    layers.Embedding(input_dim=1000, output_dim=64, input_length=len(padded_sequences[0])),\n",
        "    layers.LSTM(64),\n",
        "    layers.Dense(4, activation='softmax')\n",
        "])\n",
        "\n",
        "# Загварыг компил хийх\n",
        "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Загварыг сургалтанд оруулах\n",
        "model.fit(padded_sequences, np.array(labels), epochs=10)\n",
        "\n",
        "# Шинээр текст ангилах\n",
        "new_texts = [\"Шинжлэх ухааны том ололт\"]\n",
        "new_sequences = tokenizer.texts_to_sequences(new_texts)\n",
        "new_padded_sequences = pad_sequences(new_sequences, padding='post', maxlen=len(padded_sequences[0]))\n",
        "\n",
        "# Текстийн ангилал авах\n",
        "prediction = model.predict(new_padded_sequences)\n",
        "predicted_label = np.argmax(prediction)\n",
        "print(f\"Текстийн ангилал: {predicted_label}\")\n",
        "\n",
        "# Алдаатай үгийг тодорхойлох\n",
        "response = requests.post(\n",
        "    \"https://zuv.bichig.dev/api/check\",\n",
        "    json={\"text\": text}\n",
        ")\n",
        "\n",
        "marked_text = text  # Тэмдэглэсэн текстийг эхлээд эх текст гэж тодорхойлно.\n",
        "\n",
        "if response.status_code == 200:\n",
        "    corrections = response.json()\n",
        "    corrected_words = []\n",
        "\n",
        "    # Алдаатай үгийг доогуур зурах, санал болгох үг\n",
        "    for mistake in corrections.get(\"data\", []):\n",
        "        wrong_word = mistake[\"word\"]\n",
        "        suggestions = mistake[\"suggestions\"]\n",
        "        corrected_words.append(wrong_word)\n",
        "\n",
        "        # Алдаатай үгийн доогуур зурах\n",
        "        marked_text = re.sub(\n",
        "            rf'\\b{wrong_word}\\b',\n",
        "            f'__{wrong_word}__',\n",
        "            marked_text\n",
        "        )\n",
        "        print(f\"Алдаатай үг: {wrong_word}, Санал болгож буй үгс: {', '.join(suggestions)}\")\n",
        "else:\n",
        "    print(f\"Серверийн алдаа: {response.status_code}\")\n",
        "\n",
        "print(\"\\nТэмдэглэсэн текст:\\n\", marked_text)\n",
        "\n",
        "# Үгний үндсийг задлах\n",
        "words = word_tokenize(text)\n",
        "stemmed_words = [re.sub(r\"[^а-яА-Я]\", \"\", word.lower()) for word in words if word.isalpha()]\n",
        "word_count = Counter(stemmed_words)\n",
        "\n",
        "# Хамгийн их давтагдсан 10 үг\n",
        "most_common = word_count.most_common(10)\n",
        "print(\"\\nХамгийн их давтагдсан 10 үг:\\n\", most_common)\n",
        "\n",
        "# Текст ангилагч (англи тексттэй ажилладаг тул орчуулга шаардагдана)\n",
        "classifier = pipeline(\"zero-shot-classification\")\n",
        "\n",
        "# Ангиллын сонголтууд\n",
        "labels = [\"эдийн засаг\", \"спорт\", \"улс төр\", \"соёл урлаг\", \"технологи\"]\n",
        "\n",
        "# Бичвэрийг ангилах\n",
        "result = classifier(text, candidate_labels=labels)\n",
        "print(\"\\nБичвэрийн агуулга:\\n\", result)\n",
        "\n",
        "# Flask серверийн код\n",
        "app = Flask(__name__)\n",
        "\n",
        "@app.route('/check_text', methods=['POST'])\n",
        "def check_text():\n",
        "    data = request.get_json()\n",
        "    text = data.get('text', '')\n",
        "    response = requests.post(\"https://zuv.bichig.dev/api/check\", json={\"text\": text})\n",
        "    if response.status_code == 200:\n",
        "        corrections = response.json()\n",
        "        return jsonify(corrections)\n",
        "    return {\"error\": \"API алдаа гарлаа\"}, 500\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    app.run(debug=True)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}